---
layout: default
---
<div class="banner">
  <img src="images/banner.jpg" alt="Learning to Simulate Robot Worlds" class="banner-img">
  <div class="banner-text">
    <h1>Learning to Simulate Robot Worlds</h1>
    <h3>CoRL 2025 Workshop</h3>
    <p>September 27, 2025</p>
    <p>
      <span role="img" aria-label="South Korea Flag" style="font-size:1.2em;">ðŸ‡°ðŸ‡·</span>
      Seoul, South Korea
      <span role="img" aria-label="South Korea Flag" style="font-size:1.2em;">ðŸ‡°ðŸ‡·</span>
    </p>
  </div>
</div>
<div class="row">
  <p><b><mark>Call for Papers</mark></b>
    We are sourcing two different types of papers: four-page papers and one-page abstracts, focusing on learning to simulate robot worlds and their applications in robotics. Specific topics of interest include:
    <ul>
      <li>Real-to-Sim and Sim-to-Real Learning: How to use real-world data to create simulations and how to learn and evaluate robot policies in such simulations?</li>
      <li>Photorealistic Differentiable Simulation for Robotics: How can robotics leverage photorealistic reconstructions or digital twins?</li>
      <li>Learnable Physics Simulations and System Identification: Making physics simulation learnable and differentiable, so that robots can identify unknown parameters and even improve the simulator itself.</li>
      <li>Reconstruction and generation of articulated and deformable objects</li>
      <li>Policy training and evaluation with learned world models, realistic simulations and digital twins</li>
      <li>End-to-end world models learning</li>
      <li>Explicit vs. implicit world models</li>
      <li>Compositional world models and Controllable video generation models</li>
    </ul>
  </p>

  <p><b> Submission Policy: </b></p>
  <p>
  <ul>
    <li>Submissions are managed using OpenReview: <a href="https://openreview.net/group?id=robot-learning.org/CoRL/2025/Workshop/LSRW<">Submission site</a></li>
    <li>We encourage two types of submissions: 
      <ol>
        <li><em>Novel Papers:</em> 4-page submissions presenting new perspectives or experimental results. Should contain up to 4 pages of main paper plus any number of pages for references and supplementary material</li>
        <li><em>Abstracts from previously published work:</em> One-page abstracts of previously submitted work (e.g., from CoRL or in other related conferences). The single page should contain a summary of the paper and outline its relation to the workshop topic and a note on where the paper was presented earlier.</li>
      </ol>
    <li>We ask authors to use the supplementary material only for minor details that do not fit in the main paper. We reserve the right to desk reject papers that strongly violate this format (e.g., more than 4 pages of main content before references)</li>
    <li>4-page submissions should be fully anonymized for double-blind review.</li>
    <li>Papers should use <a href="https://github.com/cvpr-org/author-kit">this style template</a>.</li>
    <li>Accepted submissions will appear on the workshop website (non-archival).</li>
  </ul>
  </p>

  <h3>Important Dates and Links</h3>
  <p>
  <table class="table table-striped">
    <tbody>
    <tr>
      <td>Submission site opens</td>
      <td>25.07.2025</td>
    </tr>
    <tr>
      <td>Submission site</td>
      <td><a href="https://openreview.net/group?id=robot-learning.org/CoRL/2025/Workshop/LSRW<">https://openreview.net/group?id=robot-learning.org/CoRL/2025/Workshop/LSRW</a></td>
    </tr>
    <tr>
      <td>Submission deadline (4-page submissions & 1-page abstracts)</td>
      <td>18.08.2025</td>
    </tr>
    <tr>
      <td>Decisions announced</td>
      <td>05.09.2025</td>
    </tr>
    <tr>
      <td>Camera-ready due</td>
      <td>10.09.2025</td>
    </tr>
    </tbody>
    </table>
  </p>
</div>

<h3 style='margin-bottom: 12pt;'>References</h3>
<div class='references' style='font-size:11pt'>
<ul>
  <li>Patel, M., et al. "Real-to-Sim-to-Real: Bridging the Reality Gap in Robot Learning."</li>
  <li>Barcellona, L., et al. "Dreaming in 3D: Neural World Models for Robot Manipulation."</li>
  <li>Abou-Chakra, J., et al. "Real-ISSIM: Realistic Image Synthesis for Sim-to-Real Transfer."</li>
  <li>Lu, S., et al. "ManiGaussian: Manipulation with Gaussian Splatting."</li>
  <li>Xie, Y., et al. "VID2SIM: Video-to-Simulation for Robot Navigation."</li>
  <li>Yang, J., et al. "Learning to Generate and Edit Hairstyles."</li>
  <li>Agarwal, A., et al. "COSMOS: Controllable and Scalable Video Generation."</li>
  <li>Xiang, J., et al. "Structured 3D Assets for Photorealistic Rendering."</li>
  <li>Pfaff, T., et al. "Scalable Neural Rendering for Robotics."</li>
  <li>Parker-Holder, J., et al. "GENIE-2: Generative Interactive Environments."</li>
  <li>Wu, J., et al. "DayDreamer: World Models for Physical Robot Learning."</li>
  <li>Feng, L., et al. "Fine-tuning World Models for Robot Manipulation."</li>
  <li>Kerr, J., et al. "Robot Learning with Articulated Objects."</li>
  <li>Li, Y., et al. "Evaluating Robot Policies with Learned World Models."</li>
</ul>
</div>
