---
layout: default
---
<div class="banner">
  <video src="images/banner.mp4" class="banner-img" autoplay muted loop playsinline>
    <p>Your browser doesn't support HTML5 video. Here is a <a href="images/banner.mp4">link to the video</a> instead.</p>
  </video>
  <div class="banner-text">
    <h2>Learning to Simulate Robot Worlds</h2>
    <h4>CoRL 2025 Workshop</h4>
    <p>September 27, 2025</p>
    <p>
      <span role="img" aria-label="South Korea Flag" style="font-size:1.2em;">ðŸ‡°ðŸ‡·</span>
      Seoul, South Korea
      <span role="img" aria-label="South Korea Flag" style="font-size:1.2em;">ðŸ‡°ðŸ‡·</span>
    </p>
  </div>
</div>
<div class="contributed talks">
  <p>
  </br><span class="dot-invited-talk"></span> Invited Talk,  <span class="dot-panel-discussion"></span> Panel Discussion, <span class="dot-lightning-talk"></span> Contributed Talks, <span class="dot-poster-session"></span> Poster Session,  <span class="dot-other-session"></span> Other.
  </p>
  <h2>Schedule (subject to change)</h2>
  <table class="table table-striped table-condensed" id="schedule">
    <thead>
      <tr>
        <th class="time">Time</th>
        <th>Speaker/Moderator</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="dot-other-session"></span> 09:30 - 09:45</td>
        <td>Organizers</td>
        <td>Introduction</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 09:45 - 10:15</td>
        <td>Daniel Ho</td>
        <td>X World Model: Solving humanoid policy training and evaluation with data synthesis and action contro</td>
      </tr>
      <tr>
        <td><span class="dot-lightning-talk"></span> 10:15 - 10:30</td>
        <td></td>
        <td>Spotlight Presentations</td>
      </tr>
      <tr>
        <td><span class="dot-poster-session"></span> 10:30 - 11:00</td>
        <td></td>
        <td>Coffee break & Posters</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 11:00 - 11:30</td>
        <td>Hao Su</td>
        <td>Learning World Models for Embodied AI</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 11:30 - 12:00</td>
        <td>Katerina Fragkiadaki</td>
        <td>Invited Talk 3 (25 min talk + 5 min QA)</td>
      </tr>
      <tr>
        <td><span class="dot-panel-discussion"></span> 12:00 - 12:30</td>
        <td>TBD</td>
        <td>Panel Discussion</td>
      </tr>
      <tr>
        <td><span class="dot-other-session"></span> 12:30 - 13:30</td>
        <td></td>
        <td>Lunch</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 13:30 - 14:00</td>
        <td>Animesh Garg</td>
        <td>Invited Talk 4 (25 min talk + 5 min QA)</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 14:00 - 14:30</td>
        <td>Yilun Du</td>
        <td>Learning Generative World Simulators</td>
      </tr>
      <tr>
        <td><span class="dot-poster-session"></span> 14:30 - 15:00</td>
        <td></td>
        <td>Coffee Break & Posters</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 15:00 - 15:30</td>
        <td>TBD</td>
        <td>Invited Talk 6 (25 min talk + 5 min QA)</td>
      </tr>
      <tr>
        <td><span class="dot-other-session"></span> 15:30 - 15:40</td>
        <td></td>
        <td> Paper Awards</td>
      </tr>
      <tr>
        <td><span class="dot-panel-discussion"></span> 15:40 - 16:30</td>
        <td>TBD</td>
        <td>Panel Discussion</td>
      </tr>
    </tbody>
  </table>
</div>

<!--
<div id="speakers" class="row">
  <h2>Speakers and Panelists (TBD)</h2>
  <div class="break"></div>
  <div class="container" style="max-width: 1140px;">
  <div class="row align-items-start mb-4">
  
    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/animesh-garg.jpg" alt="Animesh Garg">
      <a href="https://animesh.garg.tech/">
        <h4 class="section-heading">Animesh Garg</h4>
      </a>
      <h5 class="section-heading"><i>University of Toronto, Nvidia</i></h5>
    </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Animesh Garg is the Stephen Fleming Early Career Professor in Computer Science at Georgia Tech, within the School of Interactive Computing, and is affiliated with the Robotics and Machine Learning programs. He holds courtesy appointments at the University of Toronto and the Vector Institute. Previously, he held research leadership positions at Nvidia and Apptronik. His research focuses on the algorithmic foundations of generalizable autonomy, enabling robots to acquire cognitive and dexterous skills and collaborate with humans in novel environments. His group explores structured inductive biases, causality in decision-making, multimodal object-centric representations, self-supervised learning for control, and efficient dexterous skill acquisition.</p>
   
    </div>
      <hr>

    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/daniel_ho.jpeg" alt="">
        <a href="https://itsdanielho.com/">
          <h4 class="section-heading">
            <center>Daniel Ho</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>1X Technologies</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Daniel Ho is the Director of Evaluation at 1X Technologies. His goal is to deploy generalist machines that grow from experience and correct their own mistakes. He's building World Models and large-scale evaluation pipelines towards this mission. Previously, he worked on robotics, perception, and machine learning as a Senior Software Engineer at Waymo and Everyday Robots (X, Google[X]). His research has focused on learning algorithms and representation learning to generalize ML model understanding in robotics, computer vision, and self-driving.</p>
      <p><strong>Talk Title:</strong> 1X World Model: Solving humanoid policy training and evaluation with data synthesis and action control</p>
      
    </div>
      <hr>
        <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/hao_su.jpg" alt="">
               <a href="https://cseweb.ucsd.edu/~haosu/">
          <h4 class="section-heading">
            <center>Hao Su</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>UC San Diego</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Hao Su is an Associate Professor of Computer Science at UC San Diego and Founder & CTO of Hillbot, a robotics startup. He directs the Embodied Intelligence Lab and is a founding member of the HalÄ±cÄ±oÄŸlu Data Science Institute. His research spans computer vision, machine learning, graphics, and robotics, focusing on algorithms to simulate and interact with the physical world. He holds Ph.D.s in Computer Science from Stanford and Mathematics from Beihang University. He helped develop datasets like ImageNet, ShapeNet, and tools like PointNet. Su is Program Chair of CVPR 2025 and has received NSF CAREER and SIGGRAPH awards.</p>
      <p><strong>Talk Title:</strong>Learning World Models for Embodied AI</p>

    </div>
      <hr>

    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
       src="images/speakers/katerina-fragkiadaki.png" alt="">
        <a href="https://www.cs.cmu.edu/~katef/">
          <h4 class="section-heading">
            <center>Katerina Fragkiadaki</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>Carnegie Mellon University</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Katerina Fragkiadaki is the JPMorgan Chase Associate Professor in Machine Learning at Carnegie Mellon University. She earned her B.S. from the National Technical University of Athens and her Ph.D. from the University of Pennsylvania, followed by postdoctoral work at UC Berkeley and Google Research. Her research combines common sense reasoning with deep visuomotor learning to enable few-shot and continual learning for perception, action, and language grounding. Her group develops methods in 2D-to-3D perception, vision-language grounding, and navigation policies. She received awards including the NSF CAREER and DARPA Young Investigator Awards and is Program Chair for ICLR 2024.</p>
   
    </div>
      <hr>

        <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
            src="images/speakers/yilun_du.png" alt="">
        <a href="https://yilundu.github.io/">
          <h4 class="section-heading">
            <center>Yilun Du</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>Harvard University</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Yilun Du is an Assistant Professor at Harvardâ€™s Kempner Institute and Computer Science Department, and a Senior Research Scientist at Google DeepMind. He earned his Ph.D. in EECS from MIT, advised by Leslie Kaelbling, Tomas Lozano-Perez, and Joshua Tenenbaum. He holds a bachelorâ€™s from MIT and has been a research fellow at OpenAI and a visiting researcher at FAIR and DeepMind. A gold medalist at the International Biology Olympiad, his research focuses on generative models, decision making, robot learning, and embodied agents. He develops energy-based models enabling generalization and advances in diffusion models, scene understanding, and trajectory planning.</p>
      <p><strong>Talk Title:</strong> Learning Generative World Simulators</p>
     
    </div>
     
    </div>
  </div>
</div>
-->
